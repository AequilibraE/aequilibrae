{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from uuid import uuid4\n",
    "from os import remove\n",
    "from os.path import join\n",
    "from tempfile import gettempdir\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from aequilibrae.project.database_connection import database_connection\n",
    "from aequilibrae.utils.db_utils import read_and_close\n",
    "\n",
    "from aequilibrae.transit import Transit\n",
    "from aequilibrae.utils.create_example import create_example\n",
    "\n",
    "\n",
    "# TODO:\n",
    "# 1. Update preload builder to use pce from database\n",
    "# 2. Build PC, understand how venv's and wsl system works from ground up, ubuntu 22.04, \n",
    "# 3. Remove this notebook from git."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pt_preload(\n",
    "    self, graph, start: int, end: int, inclusion_cond: str = \"start\"\n",
    ") -> np.ndarray:\n",
    "\n",
    "    # Get trip_id based on specified inclusion condition\n",
    "    with read_and_close(database_connection(\"transit\")) as conn:\n",
    "        links = pd.read_sql(build_pt_preload_sql(start, end, inclusion_cond), conn)\n",
    "\n",
    "    # Calculate non-zero preloads for each link\n",
    "    links = links.groupby(['link_id', 'direction'], as_index=False)['pce'].sum().rename(columns={\"pce\": \"preload\"})\n",
    "\n",
    "    # Merge preload onto all links/dir's in network and fill in 0 for links with no transit\n",
    "    preload = pd.merge(graph.graph, links, on=[\"link_id\", \"direction\"], how=\"left\")\n",
    "    preload[\"preload\"] = preload[\"preload\"].fillna(0)\n",
    "\n",
    "    # Extract preload sorted by __supernet_id__ (same ordering as used by capacity in assignment)\n",
    "    return preload.sort_values(by=\"__supernet_id__\")[\"preload\"].to_numpy()\n",
    "\n",
    "\n",
    "probe_point_lookup = {\n",
    "    \"start\": \"MIN(departure)\",\n",
    "    \"end\": \"MAX(arrival)\",\n",
    "    \"midpoint\": \"(MIN(departure) + MAX(arrival)) / 2\",\n",
    "}\n",
    "\n",
    "def build_pt_preload_sql(start, end, inclusion_cond):\n",
    "\n",
    "    def select_trip_ids():\n",
    "        in_period = f\"BETWEEN {start} AND {end}\"\n",
    "        if inclusion_cond == \"any\":\n",
    "            return f\"SELECT DISTINCT trip_id FROM trips_schedule WHERE arrival {in_period} OR departure {in_period}\"\n",
    "        return f\"\"\"\n",
    "            SELECT trip_id FROM trips_schedule GROUP BY trip_id\n",
    "            HAVING {probe_point_lookup[inclusion_cond]} {in_period}\n",
    "        \"\"\"\n",
    "\n",
    "    # Convert trip_id's to link/dir's via pattern_id's\n",
    "    return f\"\"\"\n",
    "        SELECT pm.link as link_id, pm.dir as direction, SUM(r.pce) as preload\n",
    "        FROM (SELECT pattern_id FROM trips WHERE trip_id IN ({select_trip_ids()})) as p \n",
    "        INNER JOIN pattern_mapping pm ON p.pattern_id = pm.pattern_id\n",
    "        INNER JOIN routes r ON p.pattern_id = r.pattern_id\n",
    "        GROUP BY pm.link, pm.dir\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/e820432c5b4a45079d3635bc17fb7c17\n"
     ]
    }
   ],
   "source": [
    "# Build temporary updated coquimbo file\n",
    "\n",
    "fldr = join(gettempdir(), uuid4().hex)\n",
    "print(fldr)\n",
    "project = create_example(fldr, \"coquimbo\")\n",
    "\n",
    "remove(join(fldr, \"public_transport.sqlite\"))\n",
    "dest_path = join(fldr, \"gtfs_coquimbo.zip\")\n",
    "\n",
    "data = Transit(project)\n",
    "\n",
    "transit = data.new_gtfs_builder(agency=\"Lisanco\", file_path=dest_path)\n",
    "\n",
    "transit.load_date(\"2016-04-13\")\n",
    "\n",
    "# Now we execute the map matching to find the real paths.\n",
    "# Depending on the GTFS size, this process can be really time-consuming.\n",
    "transit.set_allow_map_match(True)\n",
    "transit.map_match()\n",
    "\n",
    "# Finally, we save our GTFS into our model.\n",
    "transit.save_to_disk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/git/aequilibrae/aequilibrae/project/network/network.py:343: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = pd.read_sql(sql, conn).fillna(value=np.nan)\n"
     ]
    }
   ],
   "source": [
    "# Attempt to build preload with new pce column\n",
    "\n",
    "project.network.build_graphs()\n",
    "project.activate()\n",
    "\n",
    "g = project.network.graphs[\"c\"]\n",
    "g.set_skimming([\"travel_time\"])\n",
    "g.set_graph(\"travel_time\")\n",
    "g.set_blocked_centroid_flows(False)\n",
    "g.graph[\"capacity\"] = 500\n",
    "g.graph[\"travel_time\"] = g.graph[\"distance\"] / 50\n",
    "\n",
    "def hr_to_sec(e):\n",
    "    return int(e * 60 * 60)\n",
    "\n",
    "start = hr_to_sec(7)\n",
    "end = hr_to_sec(8)\n",
    "inclusion_cond = \"start\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<aequilibrae.project.network.links.Links at 0x7f6770271f60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.network.links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get trip_id based on specified inclusion condition\n",
    "with read_and_close(database_connection(\"transit\")) as conn:\n",
    "    links = pd.read_sql(build_pt_preload_sql(start, end, inclusion_cond), conn)\n",
    "\n",
    "preload = pd.merge(g.graph, links, on=[\"link_id\", \"direction\"], how=\"left\")\n",
    "preload[\"preload\"] = preload[\"preload\"].fillna(0)\n",
    "preload = preload.sort_values(by=\"__supernet_id__\")[\"preload\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preload_2 = project.network.build_pt_preload(g, hr_to_sec(start), hr_to_sec(end), inclusion_cond=\"start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12484.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(preload.sum(), preload_2.sum())\n",
    "print(len(preload), len(preload_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# project.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
